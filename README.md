
-----
# Encoder-Decoder Finetuning

This repository contains code for finetuning encoder-decoder transformer models for machine translation.

## Finetuning

You can run a quick example finetuning with the following command:

    python finetune.py --config examples/example1.json

It should create a new directory called ```examples/model-example1-v0```. After completion of the finetuning, the directory will contain the trained model, as well as the following files:

- `training.png`: Plot of the training and validation losses.
- `translations.json`: Stores the test set translations for each language pair specified in the config.
- `scores.json`: Stores the BLEU and ChrF scores of these translations with respect to the references in the test set.

The finetuning is driven by the JSON configuration file (in this case, `examples/example1.json`):

    {
        "model_dir": "examples/model-example1",
        "finetuning_parameters": {
            "base_model": "facebook/nllb-200-distilled-600M",
            "batch_size": 32,
            "num_steps": 2000
        },
        "corpora": {
            "eng_Latn": {
                "train": "examples/data/train.en",
                "dev": "examples/data/dev.en",
                "test": "examples/data/test.en",
                "permutation": 0
            },
            "tsn_Latn": {
                "train": "examples/data/train.es",
                "dev": "examples/data/dev.es",
                "test": "examples/data/test.es",
                "permutation": 1
            },
            "tso_Latn": {
                "train": "examples/data/train.es",
                "dev": "examples/data/dev.es",
                "test": "examples/data/test.es",
                "permutation": 2
            }
        },
        "bitexts": [
            {
                "src": "eng_Latn",
                "tgt": "tsn_Latn",
                "train_lines": [
                    0,
                    1024
                ]
            },
            {
                "src": "eng_Latn",
                "tgt": "tso_Latn",
                "train_lines": [
                    1024,
                    2048
                ]
            }
        ]
    }



Things of note:

- The base model should either be a path to a saved model, or the name of a model recognized by `transformers.AutoTokenizer`.
- The files in `"corpora"` should be represented in plain text, one segment per line. It is assumed that each `train` file (respectively, `dev` and `test`) has the same number of lines, and that line `k` corresponds to same meaning in each.
- Specifying permutation 0 means that no encipherment will occur. Any other permutation will randomly permute the tokens generated by the tokenizer. If you request the same permutation for two corpora, it will use the same permutation to encipher the tokens for each corpus.
- The language ids (e.g. `eng_Latn`, `tsn_Latn`, `tso_Latn`) are the language ids that will be prepended by the tokenizer to the tokenized text for that corpus. Note that we do not use `esp_Latn` in this example because we are enciphering the two Spanish corpora.


# Extracting vocabulary from a language pair dataset

This repository contains code to extract the common vocabulary appearing in the paralel corpus of two languages, e.g. en and es

## Description

`extract_vocab_version3` extracts **common phrases** (identical in both source and target languages) from a parallel corpus dataset.

---

## Input format

| Input Arg    | Type   | Description                                                                 |
|--------------|--------|-----------------------------------------------------------------------------|
| `lang_code1` | `str`  | Source language code (e.g., `"en"`, `"fr"`, `"es"`)                         |
| `lang_code2` | `str`  | Target language code (e.g., `"es"`, `"de"`, `"zh"`)                         |
| `mode`       | `str`  | Dataset type: `"train"`, `"dev"`, or `"test"` (default: `"train"`)        |

---

## Output format

- **Returns**:  
  A `List[str]` of common phrases that are exactly the same in both languages.

- **Also saves output to file**:  
  ```text
  common_vocab/{mode}.{lang_code1}_{lang_code2}_common_vocab
  ```


